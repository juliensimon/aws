{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying images with pre-trained Apache MXNet models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's download three image classification models from the Apache MXNet [model zoo](http://mxnet.io/model_zoo/).\n",
    "* **VGG-16**: the 2014 classification winner at the [ImageNet Large Scale Visual Recognition Challenge](http://image-net.org/challenges/LSVRC).\n",
    "* **Inception v3**, an evolution of GoogleNet, the 2014 winner for object detection.\n",
    "* **ResNet-152**, the 2015 winner in multiple categories.\n",
    "\n",
    "For each model, we need to download two files:\n",
    "* the **symbol** file containing the JSON definition of the neural network: layers, connections, activation functions, etc.\n",
    "* the **weights** file storing values for all neuron weights, a.k.a. parameters, learned by the network during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://data.dmlc.ml/models/imagenet/vgg/vgg16-symbol.json -O vgg16-symbol.json\n",
    "!wget http://data.dmlc.ml/models/imagenet/vgg/vgg16-0000.params -O vgg16-0000.params\n",
    "!wget http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-symbol.json -O Inception-BN-symbol.json\n",
    "!wget http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-0126.params -O Inception-BN-0000.params\n",
    "!wget http://data.dmlc.ml/models/imagenet/resnet/152-layers/resnet-152-symbol.json -O resnet-152-symbol.json\n",
    "!wget http://data.dmlc.ml/models/imagenet/resnet/152-layers/resnet-152-0000.params -O resnet-152-0000.params\n",
    "!wget http://data.dmlc.ml/models/imagenet/synset.txt -O synset.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first lines of VGG-16 symbol file. We can see the definition of the input layer ('data'), the input weights and the biases for the first convolution layer. A convolution operation is defined ('conv1_1') as well as a Rectified Linear Unit activation function ('relu1_1')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -48 vgg16-symbol.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three models have been pre-trained on the ImageNet data set which includes over 1.2 million pictures of objects and animals sorted in 1,000 categories. We can view these categories in the synset.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -10 synset.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import cv2,sys,time\n",
    "from collections import namedtuple\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "#print \"MXNet version: %s\"  % mx.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, let's load a model.\n",
    "\n",
    "First, we have to load the **weights** and **model description** from file. MXNet calls this a **checkpoint**: indeed, it's good practice to save weights after each training epoch. Once training is complete, we can look at the training log and pick the weights for the best epoch, i.e. the one with the highest validation accuracy: it's quite likely it won't be the very last one!\n",
    "\n",
    "Once loading is complete, we get a *Symbol* object and the weights, a.k.a model parameters. We then create a new *Module* and assign it the input *Symbol*. We could select the *context* where we want to run the model: the default behavior is to use a CPU context.\n",
    "\n",
    "Then, we bind the input *Symbol* to input data: we have to call it ‘data’ because that’s its name in the **input layer** of the network (remember the first few lines of the JSON file).\n",
    "\n",
    "Finally, we define the **shape** of ‘data’ as 1 x 3 x 224 x 224. ‘224 x 224’ is the image resolution, that’s how the model was trained. ‘3’ is the number of channels : red, green and blue (in this order). ‘1’ is the batch size: we’ll predict one image at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadModel(modelname, gpu=False):\n",
    "        sym, arg_params, aux_params = mx.model.load_checkpoint(modelname, 0)\n",
    "        arg_params['prob_label'] = mx.nd.array([0])\n",
    "        arg_params['softmax_label'] = mx.nd.array([0])\n",
    "        if gpu:\n",
    "            mod = mx.mod.Module(symbol=sym, context=mx.gpu(0))\n",
    "        else:\n",
    "            mod = mx.mod.Module(symbol=sym)\n",
    "        mod.bind(for_training=False, data_shapes=[('data', (1,3,224,224))])\n",
    "        mod.set_params(arg_params, aux_params)\n",
    "        return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to load the 1,000 categories stored in the synset.txt file. We'll need the actual descriptions at prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCategories():\n",
    "        synsetfile = open('synset.txt', 'r')\n",
    "        synsets = []\n",
    "        for l in synsetfile:\n",
    "                synsets.append(l.rstrip())\n",
    "        return synsets\n",
    "    \n",
    "synsets = loadCategories()\n",
    "print synsets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a function to load an image from file. Remember that the model expects a 4-dimension *NDArray* holding the red, green and blue channels of a single 224 x 224 image. We’re going to use the **OpenCV** library to build this *NDArray* from our input image.\n",
    "\n",
    "Here are the steps:\n",
    "* read the image: this will return a **numpy array** shaped as (image height, image width, 3), with the three channels in **BGR** order (blue, green and red).\n",
    "* convert the image to **RGB**.\n",
    "* resize the image to **224 x 224**.\n",
    "* **reshape** the array from (image height, image width, 3) to (3, image height, image width).\n",
    "* add a **fourth dimension** and build the *NDArray*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareNDArray(filename):\n",
    "        img = cv2.imread(filename)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (224, 224,))\n",
    "        img = np.swapaxes(img, 0, 2)\n",
    "        img = np.swapaxes(img, 1, 2)\n",
    "        img = img[np.newaxis, :]\n",
    "        array = mx.nd.array(img)\n",
    "        #print array.shape\n",
    "        return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take care of prediction. Our parameters are an image, a model, a list of categories and the number of top categories we'd like to return. \n",
    "\n",
    "Remember that a *Module* object must feed data to a model in **batches**: the common way to do this is to use a **data iterator**. Here, we’d like to predict a single image, so although we could use a data iterator, it’d probably be overkill. Instead, let's create a named tuple, called *Batch*, which will act as a fake iterator by returning our input *NDArray* when its 'data' attribute is referenced.\n",
    "\n",
    "Once the image has been forwarded, the model outputs an *NDArray* holding **1,000 probabilities**, corresponding to the 1,000 categories it has been trained on: the *NDArray* has only one line since batch size is equal to 1. \n",
    "\n",
    "Let’s turn this into an array with *squeeze()*. Then, using *argsort()*, we create a second array holding the **index** of these probabilities sorted in **descending order**. Finally, we return the top n categories and their description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(filename, model, categories, n):\n",
    "        array = prepareNDArray(filename)\n",
    "        Batch = namedtuple('Batch', ['data'])\n",
    "        t1 = time.time()\n",
    "        model.forward(Batch([array]))\n",
    "        prob = model.get_outputs()[0].asnumpy()\n",
    "        t2 = time.time()\n",
    "        print(\"Predicted in %.2f microseconds\" % (t2-t1))\n",
    "        #print prob.shape\n",
    "        prob = np.squeeze(prob)\n",
    "        sortedprobindex = np.argsort(prob)[::-1]\n",
    "        #print sortedprobindex.shape\n",
    "        \n",
    "        topn = []\n",
    "        for i in sortedprobindex[0:n]:\n",
    "                topn.append((prob[i], categories[i]))\n",
    "        return topn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to put everything together. Let's load all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(modelname, gpu=False):\n",
    "        model = loadModel(modelname,gpu)\n",
    "        categories = loadCategories()\n",
    "        return model, categories\n",
    "\n",
    "vgg16,categories = init(\"vgg16\")\n",
    "resnet152,categories = init(\"resnet-152\")\n",
    "inceptionv3,categories = init(\"Inception-BN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before classifying images, let's take a closer look to some of the VGG-16 **parameters** we just loaded from the '.params' file. First, let's print the names of all **layers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = vgg16.get_params()\n",
    "\n",
    "layers = []\n",
    "for layer in params[0].keys():\n",
    "    layers.append(layer)\n",
    "    \n",
    "layers.sort()    \n",
    "print layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you count them? There are **sixteen** total: thirteen convolutional layers and three fully connected layers. Now you know why this model is called **VGG-16** :)\n",
    "\n",
    "Now let's print the weights for the last fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print params[0]['fc8_weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice the **shape** of this matrix? **1000x4096**. This layer contains **1,000 neurons**: each of them will store the **probability** of the image belonging to a specific category. Each neuron is also fully connected to all **4,096 neurons** in the previous layer ('fc7')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, enough exploring! Now let's use these models to classify our own images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"violin.jpg\"\n",
    "\n",
    "display(Image(filename=image))\n",
    "\n",
    "topn = 5\n",
    "print (\"*** VGG16\")\n",
    "print predict(image,vgg16,categories,topn)\n",
    "print (\"*** ResNet-152\")\n",
    "print predict(image,resnet152,categories,topn)\n",
    "print (\"*** Inception v3\")\n",
    "print predict(image,inceptionv3,categories,topn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's try again with a **GPU context** this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16,categories = init(\"vgg16\", gpu=True)\n",
    "resnet152,categories = init(\"resnet-152\", gpu=True)\n",
    "inceptionv3,categories = init(\"Inception-BN\", gpu=True)\n",
    "\n",
    "print (\"*** VGG16\")\n",
    "print predict(image,vgg16,categories,topn)\n",
    "print (\"*** ResNet-152\")\n",
    "print predict(image,resnet152,categories,topn)\n",
    "print (\"*** Inception v3\")\n",
    "print predict(image,inceptionv3,categories,topn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in performance is quite noticeable: between **15x** and **20x**. If we predicted **multiple images** at the same time, the gap would widen even more due to the **massive parallelism** of GPU architectures.\n",
    "\n",
    "Now it's time to try your **own images**. Just copy them in the same folder as this notebook, update the filename in the cell above and run the predict() calls again.\n",
    "\n",
    "Have fun with pre-trained models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
